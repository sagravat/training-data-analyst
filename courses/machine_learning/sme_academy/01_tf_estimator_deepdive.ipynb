{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZhIHQQ1sYC3"
   },
   "source": [
    "# TensorFlow Estimators Deep Dive\n",
    "\n",
    "The purporse of this tutorial is to explain the details of  how to create a premade TensorFlow estimator, how trainining and evaluation work with different configurations, and how the model is exported for serving. The tutorial covers the following points:\n",
    "\n",
    "1. Implementing **Input function** with tf.data APIs.\n",
    "1. Creating **Feature columns**.\n",
    "1. Creating a **Wide and Deep** model with a premade estimator.\n",
    "4. Configuring **Train and evaluate** parameters.\n",
    "5. **Exporting** trained models for **serving**.\n",
    "6. Implementing **Early stopping**.\n",
    "7. Distribution Strategy for **multi-GPUs**.\n",
    "8. **Extending** premade estimators.\n",
    "9. Adaptive **learning rate**.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/sme_academy/01_tf_estimator_deepdive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img valign=\"middle\" src=\"images/tf-layers.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRHI7BAm3xNn"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  COLAB = True\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "RANDOM_SEED = 19831006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e1RwNw9Rr7BP",
    "outputId": "703f1591-741d-416d-c45b-8acc9765e44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow : 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "print \"TensorFlow : {}\".format(tf.__version__)\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "# print \"Eager Execution Enabled: {}\".format(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0C3sEMX1nuo"
   },
   "source": [
    "## Download Data\n",
    "\n",
    "### UCI Adult Dataset: https://archive.ics.uci.edu/ml/datasets/adult\n",
    "Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9_FhrUo81gxq",
    "outputId": "32a35c81-580c-4c77-8cb6-cd9ab0bc1640"
   },
   "outputs": [],
   "source": [
    "DATA_DIR='data'\n",
    "!mkdir $DATA_DIR\n",
    "!gsutil cp gs://cloud-samples-data/ml-engine/census/data/adult.data.csv $DATA_DIR\n",
    "!gsutil cp gs://cloud-samples-data/ml-engine/census/data/adult.test.csv $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZSRX6hl1tTM"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'adult.data.csv')\n",
    "EVAL_DATA_FILE = os.path.join(DATA_DIR, 'adult.test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sjNKmcwb1yj4",
    "outputId": "330b30e4-f1c0-4c5b-af77-aef0f54cd659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561 data/adult.data.csv\n",
      "16278 data/adult.test.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l $TRAIN_DATA_FILE\n",
    "!wc -l $EVAL_DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9q54o1g2fIF"
   },
   "source": [
    "The **training** data includes **32,561** records, while the **evaluation** data includes **16,278** records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Hgee9x2m16m9",
    "outputId": "306db090-1c1d-47ed-e194-ea9ce1ffc218"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race   gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country income_bracket  \n",
       "0          2174             0              40   United-States          <=50K  \n",
       "1             0             0              13   United-States          <=50K  \n",
       "2             0             0              40   United-States          <=50K  \n",
       "3             0             0              40   United-States          <=50K  \n",
       "4             0             0              40            Cuba          <=50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "               'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "               'native_country', 'income_bracket']\n",
    "               \n",
    "pd.read_csv(TRAIN_DATA_FILE, names=HEADER).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dYrN-U34mjk"
   },
   "source": [
    "## Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeMS4rF14Zku"
   },
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "               'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "               'native_country', 'income_bracket']\n",
    "\n",
    "HEADER_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                       [0], [0], [0], [''], ['']]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "CATEGORICAL_FEATURE_WITH_VOCABULARY = {\n",
    "  'workclass': ['State-gov', 'Self-emp-not-inc', 'Private', 'Federal-gov', 'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'], \n",
    "  'relationship': ['Not-in-family', 'Husband', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'], \n",
    "  'gender': [' Male', 'Female'], 'marital_status': [' Never-married', 'Married-civ-spouse', 'Divorced', 'Married-spouse-absent', 'Separated', 'Married-AF-spouse', 'Widowed'], \n",
    "  'race': [' White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'], \n",
    "  'education': ['Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college', 'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'], \n",
    "}\n",
    "\n",
    "CATEGORICAL_FEATURE_WITH_HASH_BUCKETS = {\n",
    "  'native_country': 60,\n",
    "  'occupation': 20\n",
    "}\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_WITH_VOCABULARY.keys() + CATEGORICAL_FEATURE_WITH_HASH_BUCKETS.keys()\n",
    "TARGET_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJk8Bryd6uXu"
   },
   "source": [
    "## 1. Data Input Function\n",
    "* Use [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) APIs: **list_files()**, **skip()**, **map()**, **filter()**, **batch()**, **shuffle()**, **repeat()**, **prefetch()**, **cache()**, etc.\n",
    "* Use [tf.data.experimental.make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset) to read and parse CSV data files.\n",
    "* Use [tf.data.experimental.make_batched_features_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_batched_features_dataset) to read and parse TFRecords data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XW3cH1qi5oxJ"
   },
   "outputs": [],
   "source": [
    "def process_features(features, target):\n",
    "  for feature_name in CATEGORICAL_FEATURE_WITH_VOCABULARY.keys() + CATEGORICAL_FEATURE_WITH_HASH_BUCKETS.keys():\n",
    "    features[feature_name] = tf.strings.strip(features[feature_name])\n",
    "  \n",
    "  features['capital_total'] = features['capital_gain'] - features['capital_loss']\n",
    "  return features, target\n",
    "\n",
    "def make_input_fn(file_pattern, batch_size, num_epochs=1, shuffle=False):\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern=file_pattern,\n",
    "            batch_size=batch_size,\n",
    "            column_names=HEADER,\n",
    "            column_defaults=HEADER_DEFAULTS,\n",
    "            label_name=TARGET_NAME,\n",
    "            field_delim=',',\n",
    "            use_quote_delim=True,\n",
    "            header=False,\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle=shuffle,\n",
    "            shuffle_buffer_size=(5 * batch_size),\n",
    "            shuffle_seed=RANDOM_SEED,\n",
    "            num_parallel_reads=multiprocessing.cpu_count(),\n",
    "            sloppy=True,\n",
    "        )\n",
    "        return dataset.map(process_features).cache()\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRYkGHG16xLU"
   },
   "outputs": [],
   "source": [
    "# You need to run tf.enable_eager_execution() at the top.\n",
    "\n",
    "dataset = make_input_fn(TRAIN_DATA_FILE, batch_size=1)()\n",
    "# for features, target in dataset.take(1):\n",
    "#   print \"Input Features:\"\n",
    "#   for key in features:\n",
    "#     print \"{}:{}\".format(key, features[key])\n",
    "\n",
    "#   print \"\"\n",
    "#   print \"Target:\"\n",
    "#   print target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM2SRlX0Gvk5"
   },
   "source": [
    "\n",
    "## 2. Create feature columns\n",
    "<br/>\n",
    "<img valign=\"middle\" src=\"images/tf-feature-columns.jpeg\" width=\"800\">\n",
    "\n",
    "Base feature columns\n",
    "  1. [numeric_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column|)\n",
    "  2. [categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list)\n",
    "  3. [categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file)\n",
    "  4. [categorical_column_with_identity](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity)\n",
    "  5. [categorical_column_with_hash_buckets](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket)\n",
    "\n",
    "Extended feature columns\n",
    "  1. [bucketized_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column)\n",
    "  2. [indicator_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column)\n",
    "  3. [crossing_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/crossed_column)\n",
    "  4. [embedding_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cI4d7VqEadF"
   },
   "outputs": [],
   "source": [
    "def create_feature_columns():\n",
    "    \n",
    "    wide_columns = []\n",
    "    deep_columns = []\n",
    "    \n",
    "    for column in NUMERIC_FEATURE_NAMES:\n",
    "        # Create numeric columns.\n",
    "        numeric_column = tf.feature_column.numeric_column(column)\n",
    "        deep_columns.append(numeric_column)\n",
    "        \n",
    "    for column in CATEGORICAL_FEATURE_WITH_VOCABULARY:\n",
    "      # Create categorical columns with vocab.\n",
    "      vocabolary = CATEGORICAL_FEATURE_WITH_VOCABULARY[column]\n",
    "      categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        column, vocabolary)\n",
    "      wide_columns.append(categorical_column)\n",
    "  \n",
    "      # Create embeddings of the categorical columns.\n",
    "      embed_size = int(math.sqrt(len(vocabolary)))\n",
    "      embedding_column = tf.feature_column.embedding_column(\n",
    "        categorical_column, embed_size)\n",
    "      deep_columns.append(embedding_column)\n",
    "\n",
    "    for column in CATEGORICAL_FEATURE_WITH_HASH_BUCKETS:\n",
    "      # Create categorical columns with hashing.\n",
    "      hash_columns = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        column, \n",
    "        hash_bucket_size=CATEGORICAL_FEATURE_WITH_HASH_BUCKETS[column])\n",
    "      wide_columns.append(hash_columns)\n",
    "\n",
    "      # Create indicators for hashing columns.\n",
    "      indicator_column = tf.feature_column.indicator_column(hash_columns) \n",
    "      deep_columns.append(indicator_column)\n",
    "\n",
    "    # Create bucktized column.\n",
    "    age_bucketized = tf.feature_column.bucketized_column(\n",
    "      deep_columns[0], boundaries = [18, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "    )\n",
    "    wide_columns.append(age_bucketized)\n",
    "\n",
    "    # Create crossing column.\n",
    "    education_X_occupation = tf.feature_column.crossed_column(\n",
    "     ['education', 'workclass'], hash_bucket_size=int(1e4))\n",
    "    wide_columns.append(education_X_occupation)\n",
    "    \n",
    "    # Create embeddings for crossing column.\n",
    "    education_X_occupation_embedded = tf.feature_column.embedding_column(\n",
    "      education_X_occupation, dimension=10)\n",
    "    deep_columns.append(education_X_occupation_embedded)\n",
    "  \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "Z7OG3ywDNIpE",
    "outputId": "814acedc-05ab-49a5-f4b7-f86434674c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wide columns:\n",
      "VocabularyListCategoricalColumn(key='workclass', vocabulary_list=('State-gov', 'Self-emp-not-inc', 'Private', 'Federal-gov', 'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'), dtype=tf.string, default_value=-1, num_oov_buckets=0)\n",
      "VocabularyListCategoricalColumn(key='relationship', vocabulary_list=('Not-in-family', 'Husband', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'), dtype=tf.string, default_value=-1, num_oov_buckets=0)\n",
      "VocabularyListCategoricalColumn(key='gender', vocabulary_list=(' Male', 'Female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)\n",
      "VocabularyListCategoricalColumn(key='marital_status', vocabulary_list=(' Never-married', 'Married-civ-spouse', 'Divorced', 'Married-spouse-absent', 'Separated', 'Married-AF-spouse', 'Widowed'), dtype=tf.string, default_value=-1, num_oov_buckets=0)\n",
      "VocabularyListCategoricalColumn(key='race', vocabulary_list=(' White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'), dtype=tf.string, default_value=-1, num_oov_buckets=0)\n",
      "VocabularyListCategoricalColumn(key='education', vocabulary_list=('Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college', 'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'), dtype=tf.string, default_value=-1, num_oov_buckets=0)\n",
      "HashedCategoricalColumn(key='native_country', hash_bucket_size=60, dtype=tf.string)\n",
      "HashedCategoricalColumn(key='occupation', hash_bucket_size=20, dtype=tf.string)\n",
      "BucketizedColumn(source_column=NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60))\n",
      "CrossedColumn(keys=('education', 'workclass'), hash_bucket_size=10000, hash_key=None)\n",
      "\n",
      "Deep columns:\n",
      "NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "NumericColumn(key='education_num', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "NumericColumn(key='capital_gain', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "NumericColumn(key='capital_loss', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "NumericColumn(key='hours_per_week', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='workclass', vocabulary_list=('State-gov', 'Self-emp-not-inc', 'Private', 'Federal-gov', 'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f864cf55c90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='relationship', vocabulary_list=('Not-in-family', 'Husband', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=2, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f864cf55a90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='gender', vocabulary_list=(' Male', 'Female'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=1, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f864cf55cd0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='marital_status', vocabulary_list=(' Never-married', 'Married-civ-spouse', 'Divorced', 'Married-spouse-absent', 'Separated', 'Married-AF-spouse', 'Widowed'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=2, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f864cf55d50>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='race', vocabulary_list=(' White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=2, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f864cf55d90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='education', vocabulary_list=('Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college', 'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f864cf55dd0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
      "IndicatorColumn(categorical_column=HashedCategoricalColumn(key='native_country', hash_bucket_size=60, dtype=tf.string))\n",
      "IndicatorColumn(categorical_column=HashedCategoricalColumn(key='occupation', hash_bucket_size=20, dtype=tf.string))\n",
      "EmbeddingColumn(categorical_column=CrossedColumn(keys=('education', 'workclass'), hash_bucket_size=10000, hash_key=None), dimension=10, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f864cf55e10>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n"
     ]
    }
   ],
   "source": [
    "wide_columns, deep_columns = create_feature_columns()\n",
    "\n",
    "print \"\"\n",
    "print \"Wide columns:\"\n",
    "for column in wide_columns:\n",
    "  print column\n",
    "\n",
    "print \"\"\n",
    "print \"Deep columns:\"\n",
    "for column in deep_columns:\n",
    "  print column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBBx7OaMPZIa"
   },
   "source": [
    "## 3. Instantiate a [Wide and Deep Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier)\n",
    "<br/>\n",
    "<img valign=\"middle\" src=\"images/dnn-wide-deep.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwdgXS-bPXlE"
   },
   "outputs": [],
   "source": [
    "def create_estimator(params, run_config):\n",
    "    \n",
    "    wide_columns, deep_columns = create_feature_columns()\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "\n",
    "        n_classes=len(TARGET_LABELS),\n",
    "        label_vocabulary=TARGET_LABELS,\n",
    "        weight_column=WEIGHT_COLUMN_NAME,\n",
    "\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_optimizer=tf.train.AdamOptimizer(\n",
    "          learning_rate=params.learning_rate),\n",
    "        dnn_hidden_units=params.hidden_units,\n",
    "        dnn_dropout=params.dropout,\n",
    "        dnn_activation_fn=tf.nn.relu,\n",
    "        batch_norm=True,\n",
    "\n",
    "        linear_feature_columns=wide_columns,\n",
    "        linear_optimizer='Adam',\n",
    "\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koqDlOh1RezR"
   },
   "source": [
    "## 4. Implement Train and Evaluate Experiment\n",
    "\n",
    "<img valign=\"middle\" src=\"images/tf-estimators.jpeg\" width=\"900\">\n",
    "\n",
    "**Delete** the **model_dir** file if you don't want a **Warm Start**\n",
    "* If not deleted, and you **change** the model, it will error.\n",
    "\n",
    "[TrainSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec)\n",
    "* Set **shuffle** in the **input_fn** to **True**\n",
    "* Set **num_epochs** in the **input_fn** to **None**\n",
    "* Set **max_steps**. One batch (feed-forward pass & backpropagation) \n",
    "corresponds to 1 training step. \n",
    "\n",
    "[EvalSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EvalSpec)\n",
    "* Set **shuffle** in the **input_fn** to **False**\n",
    "* Set Set **num_epochs** in the **input_fn** to **1**\n",
    "* Set **steps** to **None** if you want to use all the evaluation data. \n",
    "* Otherwise, set **steps** to the number of batches you want to use for evaluation, and set **shuffle** to True.\n",
    "* Set **start_delay_secs** to 0  to start evaluation as soon as a checkpoint is produced.\n",
    "* Set **throttle_secs** to 0 to re-evaluate as soon as a new checkpoint is produced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcAvhhtlOTCd"
   },
   "outputs": [],
   "source": [
    "def run_experiment(estimator, params, run_config, \n",
    "  resume=False, train_hooks=None, exporters=None):\n",
    "\n",
    "  print \"Resume training {}: \".format(resume)\n",
    "  print \"Epochs: {}\".format(epochs)\n",
    "  print \"Batch size: {}\".format(params.batch_size)\n",
    "  print \"Steps per epoch: {}\".format(steps_per_epoch)\n",
    "  print \"Training steps: {}\".format(params.max_steps)\n",
    "  print \"Learning rate: {}\".format(params.learning_rate)\n",
    "  print \"Hidden Units: {}\".format(params.hidden_units)\n",
    "  print \"Dropout probability: {}\".format(params.dropout)\n",
    "  print \"Save a checkpoint and evaluate afer {} step(s)\".format(run_config.save_checkpoints_steps)\n",
    "  print \"Keep the last {} checkpoint(s)\".format(run_config.keep_checkpoint_max)\n",
    "  print \"\"\n",
    "  \n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "  if not resume: \n",
    "    if tf.gfile.Exists(run_config.model_dir):\n",
    "      print \"Removing previous artefacts...\"\n",
    "      tf.gfile.DeleteRecursively(run_config.model_dir)\n",
    "  else:\n",
    "    print \"Resuming training...\"\n",
    "\n",
    "  # Create train specs.\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "      input_fn = make_input_fn(\n",
    "          TRAIN_DATA_FILE,\n",
    "          batch_size=params.batch_size,\n",
    "          num_epochs=None, # Run until the max_steps is reached.\n",
    "          shuffle=True\n",
    "      ),\n",
    "      max_steps=params.max_steps,\n",
    "      hooks=train_hooks\n",
    "  )\n",
    "\n",
    "  # Create eval specs.\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "      input_fn = make_input_fn(\n",
    "          EVAL_DATA_FILE,\n",
    "          batch_size=params.batch_size,     \n",
    "      ),\n",
    "      exporters=exporters,\n",
    "      start_delay_secs=0,\n",
    "      throttle_secs=0,\n",
    "      steps=None # Set to limit number of steps for evaluation.\n",
    "  )\n",
    "  \n",
    "  time_start = datetime.utcnow() \n",
    "  print \"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\"))\n",
    "  print \".......................................\"\n",
    "  \n",
    "  # Run train and evaluate.\n",
    "  tf.estimator.train_and_evaluate(\n",
    "    estimator=estimator,\n",
    "    train_spec=train_spec, \n",
    "    eval_spec=eval_spec)\n",
    "\n",
    "  time_end = datetime.utcnow() \n",
    "  print \".......................................\"\n",
    "  print \"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\"))\n",
    "  print \"\"\n",
    "  \n",
    "  time_elapsed = time_end - time_start\n",
    "  print \"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Khy9RYnfph4"
   },
   "source": [
    "## Set Parameters and Run Configurations.\n",
    "\n",
    "\n",
    "* Set **model_dir** in the **run_config**\n",
    "* If the data **size is known**, training **steps**, with respect to **epochs** would be: **(training_size / batch_size) * epochs** \n",
    "* By default, a **checkpoint** is saved every 600 secs.  That is, the model is **evaluated** only every 10mins. \n",
    "* To change this behaviour, set one of the following parameters in the **run_config**\n",
    " * **save_checkpoints_secs**: Save checkpoints every this many **seconds**.\n",
    " * **save_checkpoints_steps**: Save checkpoints every this many **steps**.\n",
    "\n",
    "* Set the number of the checkpoints to keep using **keep_checkpoint_max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUyIbhvBRWqV"
   },
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "  pass\n",
    "\n",
    "MODELS_LOCATION = 'gs://agravat-dev/models/census'\n",
    "MODEL_NAME = 'dnn_classifier'\n",
    "model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n",
    "os.environ['MODEL_DIR'] = model_dir\n",
    "\n",
    "TRAIN_DATA_SIZE = 32561\n",
    "\n",
    "params = Parameters()\n",
    "params.learning_rate = 0.001\n",
    "params.hidden_units = [128, 128, 128]\n",
    "params.dropout = 0.15\n",
    "params.batch_size =  128\n",
    "\n",
    "# Set number of steps with respect to epochs.\n",
    "epochs = 5\n",
    "steps_per_epoch = int(math.ceil(TRAIN_DATA_SIZE / params.batch_size))\n",
    "params.max_steps = steps_per_epoch * epochs\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=RANDOM_SEED,\n",
    "    save_checkpoints_steps=steps_per_epoch, # Save a checkpoint after each epoch, evaluate the model after each epoch.\n",
    "    keep_checkpoint_max=3, # Keep the 3 most recently  produced checkpoints.\n",
    "    model_dir=model_dir,\n",
    "    save_summary_steps=100, # Summary steps for Tensorboard.\n",
    "    log_step_count_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyvxkYMvtVUC"
   },
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KyG4bqn5zerp",
    "outputId": "66fa22cb-4259-46a0-936c-71837f1bc6aa"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "  from tensorboardcolab import *\n",
    "  TensorBoardColab(graph_path=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1040
    },
    "colab_type": "code",
    "id": "APPdvnMRtVZi",
    "outputId": "9211edf0-631d-4850-987d-e400563f7279"
   },
   "outputs": [],
   "source": [
    "estimator = create_estimator(params, run_config)\n",
    "run_experiment(estimator, params, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "gBsemiGotiZ-",
    "outputId": "5cea834f-31bb-4afd-b62c-e00084736fbd"
   },
   "outputs": [],
   "source": [
    "print model_dir\n",
    "!gsutil ls {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYrBHd6fDRSI"
   },
   "source": [
    "## 5. Export your trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz3jaXq8LQfc"
   },
   "source": [
    "### Implement serving input receiver function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZm-eqneDQ3M"
   },
   "outputs": [],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "  inputs = {}\n",
    "  for feature_name in FEATURE_NAMES:\n",
    "    dtype = tf.float32 if feature_name in NUMERIC_FEATURE_NAMES else tf.string\n",
    "    inputs[feature_name] = tf.placeholder(shape=[None], dtype=dtype)\n",
    "  \n",
    "  # What is wrong here? \n",
    "  return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnz1N_oQLbWj"
   },
   "source": [
    "### Export to saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAqNSl9uLbmn"
   },
   "outputs": [],
   "source": [
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "# Delete export directory if exists.\n",
    "if tf.gfile.Exists(export_dir):\n",
    "    tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "# Export the estimator as a saved_model.\n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=make_serving_input_receiver_fn()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZFsxiySNzltk",
    "outputId": "82019105-912a-4939-9fa8-79e55da04d3a"
   },
   "outputs": [],
   "source": [
    "!gsutil ls gs://agravat-dev/models/census/dnn_classifier/export/1552582374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1428
    },
    "colab_type": "code",
    "id": "-JO1gunHH6r7",
    "outputId": "2eb64e19-29ff-40ba-ed17-729791035733"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=${MODEL_DIR}/export/\n",
    "saved_model_dir=$(gsutil ls ${saved_models_base} | tail -n 1)\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-kEFV1PLJXs"
   },
   "source": [
    "### Test saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "2pN4u67aIF9A",
    "outputId": "d053425f-3014-4b2d-f12a-8f4667029039"
   },
   "outputs": [],
   "source": [
    "export_dir = os.path.join(model_dir, 'export')\n",
    "tf.gfile.ListDirectory(export_dir)[-1]\n",
    "saved_model_dir = os.path.join(export_dir, tf.gfile.ListDirectory(export_dir)[-1])\n",
    "print(saved_model_dir)\n",
    "print \"\"\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = saved_model_dir,\n",
    "    signature_def_key=\"predict\"\n",
    ")\n",
    "\n",
    "output = predictor_fn(\n",
    "    {\n",
    "        'age': [34.0],\n",
    "        'workclass': ['Private'],\n",
    "        'education': ['Doctorate'],\n",
    "        'education_num': [10.0],\n",
    "        'marital_status': ['Married-civ-spouse'],\n",
    "        'occupation': ['Prof-specialty'],\n",
    "        'relationship': ['Husband'],\n",
    "        'race': ['White'],\n",
    "        'gender': ['Male'],\n",
    "        'capital_gain': [0.0], \n",
    "        'capital_loss': [0.0], \n",
    "        'hours_per_week': [40.0],\n",
    "        'native_country':['Egyptian']\n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kOugLPBLvSR"
   },
   "source": [
    "## Export the Model during Training and Evaluation\n",
    "Saved models are exported under **<model_dir>/export/<folder_name>**.\n",
    "* [Latest Exporter](https://www.tensorflow.org/api_docs/python/tf/estimator/LatestExporter): exports a model **after each evaluation**.\n",
    " * specify the **maximum number** of exported models to keep using **exports_to_keep** param.\n",
    "* [Final Exporter](https://www.tensorflow.org/api_docs/python/tf/estimator/LatestExporter): exports only the very **last** evaluated checkpoint. of the model.\n",
    "* [Best exporter](https://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter): runs everytime when the **newly evaluted checkpoint** is **better** than any exsiting model.\n",
    " * specify the **maximum number** of exported models to keep using **exports_to_keep** param.\n",
    " * It uses the **evaluation events** stored under the **eval** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3590
    },
    "colab_type": "code",
    "id": "TXms_p48NV46",
    "outputId": "e41fa186-ab81-4361-ee89-af3f27b87d0b"
   },
   "outputs": [],
   "source": [
    "def _accuracy_bigger(best_eval_result, current_eval_result):\n",
    "  \n",
    "  metric = 'accuracy'\n",
    "  return best_eval_result[metric] < current_eval_result[metric]\n",
    "\n",
    "\n",
    "params.max_steps = 1000\n",
    "params.hidden_units = [128, 128]\n",
    "params.dropout = 0\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=RANDOM_SEED,\n",
    "    save_checkpoints_steps=200,\n",
    "    keep_checkpoint_max=1,\n",
    "    model_dir=model_dir,\n",
    "    log_step_count_steps=100\n",
    ")\n",
    "\n",
    "exporter = tf.estimator.BestExporter(\n",
    "    compare_fn=_accuracy_bigger,\n",
    "    event_file_pattern='eval_{}/*.tfevents.*'.format(datetime.utcnow().strftime(\"%H%M%S\")),\n",
    "    name=\"estimate\", # Saved models are exported under /export/estimate/\n",
    "    serving_input_receiver_fn=make_serving_input_receiver_fn(),\n",
    "    exports_to_keep=1\n",
    ")\n",
    "\n",
    "estimator = create_estimator(params, run_config)\n",
    "run_experiment(estimator, params, run_config, exporters = [exporter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "4BG6zREARkzd",
    "outputId": "72f03e5c-1c4c-4938-ac68-75e7e77760c2"
   },
   "outputs": [],
   "source": [
    "!gsutil ls {model_dir}/export/estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANWtVZMkWB07"
   },
   "source": [
    "## 6. Early Stopping\n",
    "* [stop_if_higher_hook](https://www.google.com/search?q=stop_if_higher_hook&oq=stop_if_higher_hook) \n",
    "* [stop_if_lower_hook](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_lower_hook) \n",
    "* [stop_if_no_increase_hook](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_no_increase_hook)\n",
    "* [stop_if_no_decrease_hook](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_no_decrease_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1584
    },
    "colab_type": "code",
    "id": "kNEUjWciT3b0",
    "outputId": "e5e34d19-1073-4d9a-eec6-d76798f4200e"
   },
   "outputs": [],
   "source": [
    "early_stopping_hook = tf.contrib.estimator.stop_if_no_increase_hook(\n",
    "    estimator,\n",
    "    'accuracy',\n",
    "    max_steps_without_increase=100,\n",
    "    run_every_secs=None,\n",
    "    run_every_steps=500\n",
    ")\n",
    "\n",
    "params.max_steps = 1000000\n",
    "params.hidden_units = [128, 128]\n",
    "params.dropout = 0\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=RANDOM_SEED,\n",
    "    save_checkpoints_steps=500,\n",
    "    keep_checkpoint_max=1,\n",
    "    model_dir=model_dir,\n",
    "    log_step_count_steps=100\n",
    ")\n",
    "\n",
    "run_experiment(estimator, params, run_config, exporters = [exporter], train_hooks=[early_stopping_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZx3xX6hZuNJ"
   },
   "source": [
    "## 7. Using Distribution Strategy for Utilising Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRt7d2o7Zs8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 4\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:1\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:2\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:3\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\n",
      "INFO:tensorflow:Configured nccl all-reduce.\n",
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f84fc434890>, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f84fc434a90>, '_model_dir': 'gs://agravat-dev/models/census/dnn_classifier', '_protocol': None, '_save_checkpoints_steps': 200, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 19831006, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': '', '_distribute_coordinator_mode': None}\n",
      "Resume training False: \n",
      "Epochs: 5\n",
      "Batch size: 32\n",
      "Steps per epoch: 254\n",
      "Training steps: 1000\n",
      "Learning rate: 0.001\n",
      "Hidden Units: [128, 128, 128]\n",
      "Dropout probability: 0.15\n",
      "Save a checkpoint and evaluate afer 200 step(s)\n",
      "Keep the last 5 checkpoint(s)\n",
      "\n",
      "Experiment started at 22:23:57\n",
      ".......................................\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 200 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into gs://agravat-dev/models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Initialize strategy\n",
      "INFO:tensorflow:loss = 20426060.0, step = 0\n",
      "INFO:tensorflow:global_step/sec: 5.48781\n",
      "INFO:tensorflow:loss = 9670902.0, step = 100 (17.976 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into gs://agravat-dev/models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-26T22:25:39Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://agravat-dev/models/census/dnn_classifier/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-26-22:25:53\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.8480992, accuracy_baseline = 0.76380336, auc = 0.8950923, auc_precision_recall = 0.7454656, average_loss = 0.35768998, global_step = 200, label/mean = 0.23619665, loss = 2166912.0, precision = 0.73177767, prediction/mean = 0.28320998, recall = 0.56338596\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: gs://agravat-dev/models/census/dnn_classifier/model.ckpt-200\n",
      "INFO:tensorflow:Performing best model export.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from gs://agravat-dev/models/census/dnn_classifier/model.ckpt-200\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://agravat-dev/models/census/dnn_classifier/export/estimate/temp-1553639154/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 1.74598\n",
      "INFO:tensorflow:loss = 12723923.0, step = 200 (57.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.50683\n",
      "INFO:tensorflow:loss = 8142250.0, step = 300 (10.519 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into gs://agravat-dev/models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-26T22:26:47Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://agravat-dev/models/census/dnn_classifier/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-26-22:27:01\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.8534586, accuracy_baseline = 0.76380336, auc = 0.9061032, auc_precision_recall = 0.7627552, average_loss = 0.32659045, global_step = 400, label/mean = 0.23619665, loss = 1978508.8, precision = 0.7616383, prediction/mean = 0.24377924, recall = 0.5524793\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: gs://agravat-dev/models/census/dnn_classifier/model.ckpt-400\n",
      "INFO:tensorflow:Performing best model export.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from gs://agravat-dev/models/census/dnn_classifier/model.ckpt-400\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://agravat-dev/models/census/dnn_classifier/export/estimate/temp-1553639223/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 1.67071\n",
      "INFO:tensorflow:loss = 6789484.5, step = 400 (59.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.45004\n",
      "INFO:tensorflow:No increase in metric \"accuracy\" for 101 steps, which is greater than or equal to max steps (100) configured for early stopping.\n",
      "INFO:tensorflow:Requesting early stopping at global step 500\n",
      "INFO:tensorflow:loss = 7696302.0, step = 500 (11.789 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 501 into gs://agravat-dev/models/census/dnn_classifier/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-26T22:27:46Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://agravat-dev/models/census/dnn_classifier/model.ckpt-501\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-26-22:28:00\n",
      "INFO:tensorflow:Saving dict for global step 501: accuracy = 0.85677797, accuracy_baseline = 0.76380336, auc = 0.9073559, auc_precision_recall = 0.76535296, average_loss = 0.3242458, global_step = 501, label/mean = 0.23619665, loss = 1964304.9, precision = 0.7294636, prediction/mean = 0.2634955, recall = 0.6256721\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 501: gs://agravat-dev/models/census/dnn_classifier/model.ckpt-501\n",
      "INFO:tensorflow:Performing best model export.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'Placeholder_30:0' shape=(?,) dtype=float32>, 'workclass': <tf.Tensor 'Placeholder_31:0' shape=(?,) dtype=string>, 'relationship': <tf.Tensor 'Placeholder_32:0' shape=(?,) dtype=string>, 'gender': <tf.Tensor 'Placeholder_33:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'marital_status': <tf.Tensor 'Placeholder_34:0' shape=(?,) dtype=string>, 'race': <tf.Tensor 'Placeholder_35:0' shape=(?,) dtype=string>, 'capital_gain': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'native_country': <tf.Tensor 'Placeholder_37:0' shape=(?,) dtype=string>, 'capital_loss': <tf.Tensor 'Placeholder_29:0' shape=(?,) dtype=float32>, 'education': <tf.Tensor 'Placeholder_36:0' shape=(?,) dtype=string>, 'education_num': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'occupation': <tf.Tensor 'Placeholder_38:0' shape=(?,) dtype=string>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from gs://agravat-dev/models/census/dnn_classifier/model.ckpt-501\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://agravat-dev/models/census/dnn_classifier/export/estimate/temp-1553639281/saved_model.pb\n",
      "INFO:tensorflow:Finalize strategy.\n",
      "INFO:tensorflow:Loss for final step: 7696302.0.\n",
      ".......................................\n",
      "Experiment finished at 22:28:20\n",
      "\n",
      "Experiment elapsed time: 263.423971 seconds\n"
     ]
    }
   ],
   "source": [
    "strategy = None\n",
    "\n",
    "\n",
    "params.max_steps = 1000\n",
    "params.learning_rate = 0.001\n",
    "params.hidden_units = [128, 128, 128]\n",
    "params.dropout = 0.15\n",
    "params.batch_size =  128\n",
    "\n",
    "\n",
    "num_gpus = len([device_name for device_name in tf.contrib.eager.list_devices()\n",
    "                if '/device:GPU' in device_name])\n",
    "\n",
    "print \"GPUs available: {}\".format(num_gpus)\n",
    "\n",
    "if num_gpus > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    params.batch_size = int(math.ceil(params.batch_size / num_gpus))\n",
    "\n",
    "    \n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=RANDOM_SEED,\n",
    "    save_checkpoints_steps=200,\n",
    "    model_dir=model_dir,\n",
    "    train_distribute=strategy\n",
    ")\n",
    "\n",
    "estimator = create_estimator(params, run_config)\n",
    "\n",
    "early_stopping_hook = tf.contrib.estimator.stop_if_no_increase_hook(\n",
    "    estimator,\n",
    "    'accuracy',\n",
    "    max_steps_without_increase=100,\n",
    "    run_every_secs=None,\n",
    "    run_every_steps=500\n",
    ")\n",
    "\n",
    "\n",
    "run_experiment(estimator, params, run_config, exporters = [exporter], train_hooks=[early_stopping_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Jc0FsSpcxSD"
   },
   "source": [
    "## 8. Extending a Premade Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMzijRO-c_2_"
   },
   "source": [
    "### Add an evaluation metric\n",
    "* [tf.metrics](https://www.tensorflow.org/api_docs/python/tf/metrics)\n",
    "* [tf.estimator.add_metric](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/add_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1074
    },
    "colab_type": "code",
    "id": "C-1h1WYkY8FJ",
    "outputId": "961b6911-e9d6-4c1c-92c3-598ec2585edb"
   },
   "outputs": [],
   "source": [
    "def metric_fn(labels, predictions):\n",
    "\n",
    "  metrics = {}\n",
    "  \n",
    "  label_index = tf.contrib.lookup.index_table_from_tensor(tf.constant(TARGET_LABELS)).lookup(labels)\n",
    "  one_hot_labels = tf.one_hot(label_index, len(TARGET_LABELS))\n",
    "\n",
    "  metrics['mirco_accuracy'] = tf.metrics.mean_per_class_accuracy(\n",
    "    labels=label_index,\n",
    "    predictions=predictions['class_ids'],\n",
    "    num_classes=2)\n",
    "  \n",
    "  metrics['f1_score'] = tf.contrib.metrics.f1_score(\n",
    "    labels=one_hot_labels,\n",
    "    predictions=predictions['probabilities'])\n",
    "\n",
    "  return metrics\n",
    "\n",
    "params.max_steps = 1\n",
    "estimator = create_estimator(params, run_config)\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, metric_fn)\n",
    "run_experiment(estimator, params, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7KfzoD2j_JE"
   },
   "source": [
    "### Add Forward Features\n",
    "* [tf.estimator.forward_features](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/forward_features)\n",
    "This is very useful for batch prediction, in order to make instances to their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "GOn0O0w9eXzm",
    "outputId": "acbab984-73ef-470d-a15d-f2992ec810ac"
   },
   "outputs": [],
   "source": [
    "estimator = tf.contrib.estimator.forward_features(estimator, keys=\"row_identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "ERHxBSHsklWw",
    "outputId": "80bd10b9-ff41-4ae6-882d-df0f05726a09"
   },
   "outputs": [],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "  inputs = {}\n",
    "  for feature_name in FEATURE_NAMES:\n",
    "    dtype = tf.float32 if feature_name in NUMERIC_FEATURE_NAMES else tf.string\n",
    "    inputs[feature_name] = tf.placeholder(shape=[None], dtype=dtype)\n",
    "  \n",
    "  processed_inputs,_ = process_features(inputs, None)\n",
    "  processed_inputs['row_identifier'] = tf.placeholder(shape=[None], dtype=tf.string)\n",
    "  return tf.estimator.export.build_raw_serving_input_receiver_fn(processed_inputs)\n",
    "\n",
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "    tf.gfile.DeleteRecursively(export_dir)\n",
    "        \n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=make_serving_input_receiver_fn()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1564
    },
    "colab_type": "code",
    "id": "CTvGsBEBksR0",
    "outputId": "86b87a08-d6e1-42da-a564-993ad73ff31d"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=${MODEL_DIR}/export/\n",
    "saved_model_dir=$(gsutil ls ${saved_models_base} | tail -n 1)\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "J0xhpPMzmHLG",
    "outputId": "aa8128ae-6dfa-4647-e135-574e99557c1f"
   },
   "outputs": [],
   "source": [
    "export_dir = os.path.join(model_dir, 'export')\n",
    "tf.gfile.ListDirectory(export_dir)[-1]\n",
    "saved_model_dir = os.path.join(export_dir, tf.gfile.ListDirectory(export_dir)[-1])\n",
    "print(saved_model_dir)\n",
    "print \"\"\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = saved_model_dir,\n",
    "    signature_def_key=\"predict\"\n",
    ")\n",
    "\n",
    "output = predictor_fn(\n",
    "    {   'row_identifier': ['key0123'],\n",
    "        'age': [34.0],\n",
    "        'workclass': ['Private'],\n",
    "        'education': ['Doctorate'],\n",
    "        'education_num': [10.0],\n",
    "        'marital_status': ['Married-civ-spouse'],\n",
    "        'occupation': ['Prof-specialty'],\n",
    "        'relationship': ['Husband'],\n",
    "        'race': ['White'],\n",
    "        'gender': ['Male'],\n",
    "        'capital_gain': [0.0], \n",
    "        'capital_loss': [0.0], \n",
    "        'hours_per_week': [40.0],\n",
    "        'native_country':['Egyptian']\n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVfpA-NN3o5G"
   },
   "source": [
    "## 9. Adaptive learning rate\n",
    "* [exponential_decay](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay)\n",
    "* [consine_decay](https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay)\n",
    "* [linear_cosine_decay](https://www.tensorflow.org/api_docs/python/tf/train/linear_cosine_decay)\n",
    "* [consine_decay_restarts](https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay_restarts)\n",
    "* [polynomial decay](https://www.tensorflow.org/api_docs/python/tf/train/polynomial_decay)\n",
    "* [piecewise_constant_decay](https://www.tensorflow.org/api_docs/python/tf/train/piecewise_constant_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_N9HocamU5b"
   },
   "outputs": [],
   "source": [
    "def create_estimator(params, run_config):\n",
    "  \n",
    "  wide_columns, deep_columns = create_feature_columns()\n",
    "  \n",
    "  def _update_optimizer(initial_learning_rate, decay_steps):\n",
    "    \n",
    "    # learning_rate = tf.train.exponential_decay(\n",
    "    #   initial_learning_rate,\n",
    "    #   global_step=tf.train.get_global_step(),\n",
    "    #   decay_steps=decay_steps,\n",
    "    #   decay_rate=0.9\n",
    "    # )\n",
    "\n",
    "    learning_rate = tf.train.cosine_decay_restarts(\n",
    "      initial_learning_rate,\n",
    "      tf.train.get_global_step(),\n",
    "      first_decay_steps=50,\n",
    "      t_mul=2.0,\n",
    "      m_mul=1.0,\n",
    "      alpha=0.0,\n",
    "    )\n",
    "\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    return tf.train.AdamOptimizer(\n",
    "      learning_rate=initial_learning_rate)\n",
    "    \n",
    "  estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "\n",
    "    n_classes=len(TARGET_LABELS),\n",
    "    label_vocabulary=TARGET_LABELS,\n",
    "    weight_column=WEIGHT_COLUMN_NAME,\n",
    "\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_optimizer=lambda: _update_optimizer(params.learning_rate, params.max_steps),\n",
    "    dnn_hidden_units=params.hidden_units,\n",
    "    dnn_dropout=params.dropout,\n",
    "    batch_norm=True,\n",
    "    \n",
    "    linear_feature_columns=wide_columns,\n",
    "    linear_optimizer='Adam',\n",
    "    \n",
    "    config=run_config\n",
    "  )\n",
    "  \n",
    "  return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2128
    },
    "colab_type": "code",
    "id": "oSZLQLaz4FQ_",
    "outputId": "c2cee42e-cb8f-4ed8-e4a8-b01ae45981f6"
   },
   "outputs": [],
   "source": [
    "params.learning_rate = 0.1\n",
    "params.max_steps = 1000\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=RANDOM_SEED,\n",
    "    save_checkpoints_steps=200,\n",
    "    model_dir=model_dir,\n",
    ")\n",
    "\n",
    "if COLAB:\n",
    "  from tensorboardcolab import *\n",
    "  TensorBoardColab(graph_path=model_dir)\n",
    "\n",
    "estimator = create_estimator(params, run_config)\n",
    "run_experiment(estimator, params, run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1bahMKz848A"
   },
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5hU3tDV85FP"
   },
   "source": [
    "Author: Khalid Salama\n",
    "\n",
    "---\n",
    "**Disclaimer**: This is not an official Google product. The sample code provided for an educational purpose.\n",
    "\n",
    "---\n",
    "\n",
    "Copyright 2019 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0kOugLPBLvSR",
    "ANWtVZMkWB07",
    "PZx3xX6hZuNJ",
    "2Jc0FsSpcxSD",
    "KMzijRO-c_2_",
    "p7KfzoD2j_JE",
    "QVfpA-NN3o5G"
   ],
   "name": "01-tf_estimator_deepdive.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
